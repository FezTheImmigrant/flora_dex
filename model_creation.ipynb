{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "precise-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import H5pyHelper\n",
    "from tensorflow.keras import layers, models, regularizers, optimizers\n",
    "import dask.array as da\n",
    "from dask.array.slicing import shuffle_slice\n",
    "import cv2\n",
    "#from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "duplicate-newspaper",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  0\n",
      "[[[ 0.37635422  0.4665503   0.29400128]\n",
      "  [ 0.38005617  0.46284834  0.29770324]\n",
      "  [ 0.38137382  0.45541304  0.29666793]\n",
      "  ...\n",
      "  [-0.145685   -0.12215561 -0.1306262 ]\n",
      "  [-0.19227324 -0.19227324 -0.19227324]\n",
      "  [-0.19227324 -0.19227324 -0.19227324]]\n",
      "\n",
      " [[ 0.3689503   0.47025225  0.29029933]\n",
      "  [ 0.37090492  0.46567664  0.29487494]\n",
      "  [ 0.37160066  0.4583746   0.29548332]\n",
      "  ...\n",
      "  [ 0.01164832  0.13335373  0.08927029]\n",
      "  [-0.10752796 -0.05510819 -0.07432877]\n",
      "  [-0.19227324 -0.19227324 -0.19227324]]\n",
      "\n",
      " [[ 0.35972676  0.474864    0.2885111 ]\n",
      "  [ 0.35994887  0.46931106  0.2927313 ]\n",
      "  [ 0.3604797   0.4622146   0.29419577]\n",
      "  ...\n",
      "  [ 0.14725304  0.35416126  0.2793503 ]\n",
      "  [-0.03427355  0.0634582   0.02762323]\n",
      "  [-0.19227324 -0.19227324 -0.19227324]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.03603794 -0.07619481 -0.09188109]\n",
      "  [-0.03137347 -0.07382555 -0.09091858]\n",
      "  [-0.02751089 -0.07194697 -0.09014305]\n",
      "  ...\n",
      "  [ 0.37748364  0.50570947  0.30167502]\n",
      "  [ 0.38046905  0.51076174  0.30225477]\n",
      "  [ 0.38372678  0.5156483   0.30388364]]\n",
      "\n",
      " [[-0.04347324 -0.08268893 -0.09009285]\n",
      "  [-0.03889761 -0.0809416  -0.09117383]\n",
      "  [-0.03444547 -0.07918274 -0.09260517]\n",
      "  ...\n",
      "  [ 0.36901304  0.5052427   0.29348302]\n",
      "  [ 0.37221304  0.5097912   0.30000573]\n",
      "  [ 0.375915    0.5133895   0.305766  ]]\n",
      "\n",
      " [[-0.0471752  -0.08639089 -0.08639089]\n",
      "  [-0.04347324 -0.08639089 -0.09009285]\n",
      "  [-0.03933206 -0.0859203  -0.09423403]\n",
      "  ...\n",
      "  [ 0.36160913  0.50435424  0.28474638]\n",
      "  [ 0.36480913  0.5078366   0.29651108]\n",
      "  [ 0.36851108  0.50968754  0.305766  ]]]\n",
      "[ 2350 23254 20070 ...   860 15795 23654]\n",
      "dask.array<getitem, shape=(24579, 500, 500, 3), dtype=float32, chunksize=(100, 500, 500, 3), chunktype=numpy.ndarray>\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "hf = h5py.File('/workspaces/flora_dex/h5_files/copies/data.h5', 'r')\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = hf['y_train'][0].shape[0] ## gets number of classes\n",
    "INP_SHAPE = hf['x_train'][0].shape\n",
    "TRAIN_SIZE = hf[\"x_train\"].shape[0]\n",
    "TEST_SIZE = hf[\"x_test\"].shape[0]\n",
    "\n",
    "lol = hf[\"x_train\"]\n",
    "x = da.from_array(lol,chunks=(100,500,500,3))\n",
    "#x = da.from_array(lol)\n",
    "\n",
    "print (lol[0])\n",
    "\n",
    "np.random.seed(42)\n",
    "index = np.random.choice(TRAIN_SIZE,TRAIN_SIZE,replace=False)\n",
    "print (index)\n",
    "d_arry = shuffle_slice(x,index)\n",
    "\n",
    "print (d_arry)\n",
    "\n",
    "#x_new = d_arry.compute()\n",
    "\n",
    "#print (x_new[0])\n",
    "\n",
    "\n",
    "hf.close()\n",
    "\n",
    "pics = []\n",
    "for i in range(1):\n",
    "    for j in range(32):\n",
    "        pics.append(cv2.imread(f\"/workspaces/flora_dex/raw_data/Abronia elliptica/{j}.jpg\"))\n",
    "    print(i)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[56 66 31  7 55 61 65 84 54 28 23 12 35 37 59 62 16 91 10 98 95 90 13 60\n",
      " 29 87 85 43 69  6  0 89 34  1 94 14 17 93 63 48 49 86 68 32 82 51 58  4\n",
      "  5  2 39 15  8 27 57 11 76 50 30 22 99 26 78 80 47 97 71 74 46 52 75 44\n",
      " 64 96 41 40 67 25 38 70 53 83 45  9 18  3 36 73 20 72 88 21 81 79 19 42\n",
      " 24 77 92 33]\n",
      "dask.array<array, shape=(100, 500, 500, 3), dtype=float64, chunksize=(100, 125, 125, 3), chunktype=numpy.ndarray>\n",
      "[[[56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  ...\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]]\n",
      "\n",
      " [[56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  ...\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]]\n",
      "\n",
      " [[56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  ...\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  ...\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]]\n",
      "\n",
      " [[56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  ...\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]]\n",
      "\n",
      " [[56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  ...\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]\n",
      "  [56. 56. 56.]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base = []\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    l = np.ones((500,500,3)) * i\n",
    "    base.append(l)\n",
    "\n",
    "base = np.array(base)\n",
    "\n",
    "#print (base)\n",
    "\n",
    "\n",
    "#x = da.from_array(base ,chunks= (20,500,500,3))\n",
    "x = da.from_array(base)\n",
    "\n",
    "index = np.random.choice(100,100,replace=False)\n",
    "\n",
    "print(index)\n",
    "print (x)\n",
    "d_arry = shuffle_slice(x,index)\n",
    "\n",
    "x_new = d_arry.compute()\n",
    "\n",
    "print (x_new[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "forced-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A memory-mapped array is kept on disk. However, it can be accessed and sliced like any ndarray.\n",
    "# Memory mapping is especially useful for accessing small fragments of large files without reading\n",
    "# the entire file into memory.\n",
    "\n",
    "#def generator(feature_name,label_name,batch_size,shuffle):\n",
    "#    index = 0\n",
    "#    \n",
    "#    hf = h5py.File('/home/sorozco0612/dev/flora_dex/raw_data/data.h5', 'a')\n",
    "#    \n",
    "#    # shuffle data\n",
    "#    if (shuffle):\n",
    "#        print(\"Shuffling data before starting training...\")\n",
    "#        #random.seed(datetime.now())\n",
    "#\n",
    "#        #random.shuffle(hf[feature_name])\n",
    "#        #random.shuffle(hf[label_name])\n",
    "#    \n",
    "#    while True:\n",
    "#        if index == 0:\n",
    "#    \n",
    "#            x = hf[feature_name]\n",
    "#            y = hf[label_name] \n",
    "#            \n",
    "#            ## create shuffle index for each batch\n",
    "#            idx_map = np.arange(x.shape[0])\n",
    "#            np.random.shuffle(idx_map)\n",
    "#        \n",
    "#        # batch has not met the end of the data\n",
    "#        if (index + batch_size < x.shape[0]):\n",
    "#            batch = sorted(idx_map[index:index+batch_size])\n",
    "#            features = x[batch]\n",
    "#            labels = y[batch]\n",
    "np.append(base,x)\n",
    "#\n",
    "#            index += batch_size\n",
    "#        else:\n",
    "#            # batch size will be smaller than the rest on last iteration \n",
    "#            \n",
    "#            batch = sorted(idx_map[index:])\n",
    "#            features = x[batch]\n",
    "#            labels = y[batch]\n",
    "#            \n",
    "#            idx_map = np.arange(features.shape[0])\n",
    "#            np.random.shuffle(idx_map)\n",
    "#\n",
    "#            index = 0\n",
    "#            \n",
    "#            ## close file so it can be reshuffled\n",
    "#            hf.close()\n",
    "#        \n",
    "#        ## shuffle just the batches\n",
    "#        #features = features[idx_map]\n",
    "#        #labels = labels[idx_map]\n",
    "#        \n",
    "#        yield (features,labels)\n",
    "        \n",
    "        \n",
    "def generator(feature_name,label_name,batch_size,shuffle):\n",
    "    index = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (shuffle):\n",
    "        pass\n",
    "        #print(\"Shuffling data before starting training...\")\n",
    "        #seed = datetime.now()\n",
    "        \n",
    "        #H5pyHelper.shuffle_dataset(\n",
    "        #            \"/home/sorozco0612/dev/flora_dex/raw_data/data.h5\", feature_name, seed\n",
    "        #        )\n",
    "        #H5pyHelper.shuffle_dataset(\n",
    "        #           \"/home/sorozco0612/dev/flora_dex/raw_data/data.h5\", label_name, seed\n",
    "        #        )\n",
    "    while True:\n",
    "        features = []\n",
    "        if index == 0:\n",
    "            hf = h5py.File('/workspaces/flora_dex/raw_data/data_segmented.h5', 'r')\n",
    "    \n",
    "            x = hf[feature_name]\n",
    "            y = hf[label_name]  \n",
    "        \n",
    "        # batch has not met the end of the data\n",
    "        if (index + batch_size < x.shape[0]):\n",
    "            for j in range(32):\n",
    "                img = cv2.imread(f\"/workspaces/flora_dex/raw_data/Abronia elliptica/{j}.jpg\")\n",
    "                img = cv2.resize(img,(500,500))\n",
    "                features.append(img)\n",
    "            #features = x[index:index+batch_size]\n",
    "            labels = y[index:index+batch_size]\n",
    "            index += batch_size\n",
    "        else:\n",
    "            # batch size will be smaller than the rest on last iteration\n",
    "            features = x[index:]\n",
    "            labels = y[index:]\n",
    "            index = 0\n",
    "            \n",
    "            ## close file so it can be reshuffled\n",
    "            hf.close()\n",
    "        \n",
    "        features = np.asarray(features)\n",
    "            \n",
    "        yield (features,labels)   \n",
    "\n",
    "train_generator = generator('x_train', 'y_train',BATCH_SIZE,True)\n",
    "test_generator = generator('x_test','y_test',BATCH_SIZE,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "interracial-identity",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_9\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_45 (Conv2D)           (None, 123, 123, 96)      34944     \n_________________________________________________________________\nbatch_normalization_45 (Batc (None, 123, 123, 96)      384       \n_________________________________________________________________\nmax_pooling2d_27 (MaxPooling (None, 61, 61, 96)        0         \n_________________________________________________________________\nconv2d_46 (Conv2D)           (None, 61, 61, 256)       614656    \n_________________________________________________________________\nbatch_normalization_46 (Batc (None, 61, 61, 256)       1024      \n_________________________________________________________________\nmax_pooling2d_28 (MaxPooling (None, 30, 30, 256)       0         \n_________________________________________________________________\nconv2d_47 (Conv2D)           (None, 30, 30, 384)       885120    \n_________________________________________________________________\nbatch_normalization_47 (Batc (None, 30, 30, 384)       1536      \n_________________________________________________________________\nconv2d_48 (Conv2D)           (None, 30, 30, 384)       1327488   \n_________________________________________________________________\nbatch_normalization_48 (Batc (None, 30, 30, 384)       1536      \n_________________________________________________________________\nconv2d_49 (Conv2D)           (None, 30, 30, 256)       884992    \n_________________________________________________________________\nbatch_normalization_49 (Batc (None, 30, 30, 256)       1024      \n_________________________________________________________________\nmax_pooling2d_29 (MaxPooling (None, 14, 14, 256)       0         \n_________________________________________________________________\nflatten_9 (Flatten)          (None, 50176)             0         \n_________________________________________________________________\ndense_27 (Dense)             (None, 4096)              205524992 \n_________________________________________________________________\ndropout_18 (Dropout)         (None, 4096)              0         \n_________________________________________________________________\ndense_28 (Dense)             (None, 4096)              16781312  \n_________________________________________________________________\ndropout_19 (Dropout)         (None, 4096)              0         \n_________________________________________________________________\ndense_29 (Dense)             (None, 102)               417894    \n=================================================================\nTotal params: 226,476,902\nTrainable params: 226,474,150\nNon-trainable params: 2,752\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## alex net\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(500,500,3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4096, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "random-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001), metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer=tf.optimizers.SGD(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "understanding-decrease",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/250\n",
      " 32/768 [>.............................] - ETA: 41:44 - loss: 3.1871 - accuracy: 0.5566"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-1ada29d49039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 epochs=250)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "fit = model.fit(x=train_generator,\n",
    "                steps_per_epoch= TRAIN_SIZE // BATCH_SIZE,\n",
    "                callbacks=[callback], \n",
    "                validation_steps= TEST_SIZE // BATCH_SIZE,\n",
    "                validation_data=test_generator,\n",
    "                verbose=1,\n",
    "                epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_prediction(Y_true, Y_pred):\n",
    "    #mse = mean_squared_error(Y_true, Y_pred)\n",
    "    #accuracy = accuracy_score(Y_true, Y_pred)\n",
    "    #print(f'mse       = {mse:.2}')\n",
    "    print(f'accuracy = {accuracy:.2%}')\n",
    "    \n",
    "def predict_and_summarize(X, Y):\n",
    "    Y_pred = model.predict(X)\n",
    "    #summarize_prediction(Y, Y_pred)\n",
    "    return Y_pred\n",
    "\n",
    "hf = h5py.File('/home/sorozco0612/dev/flora_dex/raw_data/data.h5', 'r')\n",
    "x_test = hf[\"x_test\"]\n",
    "y_test = hf[\"y_test\"]\n",
    "\n",
    "y_pred = predict_and_summarize(x_test, y_test)\n",
    "\n",
    "print(y_pred[:1])\n",
    "print(y_test[:1])\n",
    "\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-employer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}